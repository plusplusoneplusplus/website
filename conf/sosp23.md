## Efficient Memory Management for Large Language Model Serving with PagedAttention (UC Berkeley)

### Main Innovation
This paper introduces **PagedAttention**, a novel attention algorithm that restructures how the **key-value (KV) cache** used in autoregressive LLM inference is stored and managed. Instead of allocating large contiguous memory slabs for each request’s KV cache, PagedAttention breaks the cache into fixed-size blocks (pages) that can be placed anywhere in memory, inspired by operating-system virtual memory and paging. Logical-to-physical block mappings enable on-demand allocation, minimal internal/external fragmentation, and flexible sharing of KV blocks across requests or beams using **copy-on-write** semantics. On top of this algorithm, the authors build **vLLM**, an LLM serving system that tightly integrates block-level memory management with scheduling to eliminate wasted memory and support larger batch sizes.  [oai_citation:0‡arXiv](https://arxiv.org/abs/2309.06180?utm_source=chatgpt.com)

### Academic Significance
The work reframes memory management in transformer inference by adapting classical systems ideas (paging/virtual memory) to deep learning serving, addressing a key bottleneck in high-throughput LLM serving. It quantifies inefficiencies in existing contiguous allocation schemes and shows how fine-grained block management can nearly eliminate fragmentation and enable KV cache sharing—concepts that extend beyond this specific implementation. This opens new directions for memory-efficient architectures in model serving and promotes cross-disciplinary techniques bridging systems research with ML infrastructure.  [oai_citation:1‡ar5iv](https://ar5iv.labs.arxiv.org/html/2309.06180?utm_source=chatgpt.com)

### Industry Significance
Practically, PagedAttention and vLLM dramatically increase serving throughput (2–4×) without compromising latency or model quality by reducing GPU memory waste and enabling much larger effective batch sizes, especially for long contexts and complex decoding (beam search, parallel sampling). These gains translate to lower infrastructure costs and better scalability for real-world LLM services and APIs. The design also eases memory pressure on accelerators, enabling more efficient utilization of existing hardware in cloud and edge deployments.  [oai_citation:2‡arXiv](https://arxiv.org/abs/2309.06180?utm_source=chatgpt.com)