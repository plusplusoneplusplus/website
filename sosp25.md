# Operating Systems and Virtualization

LithOS: An Operating System for Efficient Machine Learning on GPUs. This paper introduces a new OS, LithOS, designed to optimize GPU utilization for machine learning workloads ￼. LithOS rearchitects GPU resource management with a per-core (TPC) scheduler and a “kernel atomizer” that breaks GPU kernels into smaller units to reduce blocking and enable dynamic resource reallocation ￼. It also implements lightweight hardware right-sizing and power management mechanisms to match GPU resources to workload demands ￼. Together, these innovations allow multiple ML tasks to efficiently share GPUs, improving throughput and reducing interference.

μFork: Supporting POSIX fork Within a Single-Address-Space OS. The authors present μFork, a single-address-space OS design that emulates POSIX processes and implements the fork() call without traditional virtual memory ￼. The key novelty is copying a process’s memory state at μprocess granularity, avoiding full address space duplication. μFork achieves efficient, isolated process creation in a single address space without compromising compatibility with multiprocess POSIX applications ￼. This allows modern hardware to benefit from single-address-space OS performance while still supporting legacy fork() semantics.

Tock: From Research to Securing 10 Million Computers. This work reports on the transition of the Tock operating system from a research prototype to a widely deployed platform. Tock is an embedded OS focused on safety and isolation for microcontrollers. The paper’s contribution lies in the pragmatic engineering required to secure millions of real-world devices using Tock. It details enhancements made to meet production requirements, such as memory safety and fault isolation improvements, demonstrating how a research OS can evolve to secure large-scale deployments (over 10 million devices) ￼. This experience provides insight into bridging academic design principles with industry-scale security needs.

Proto: A Guided Journey through Modern OS Construction. Proto is a pedagogical OS that distills modern OS principles into a concise implementation ￼. The paper’s contribution is a clean-slate OS design used to illustrate key concepts in OS construction, such as process isolation, scheduling, and device management, in a unified framework. By walking through Proto’s design, the authors demonstrate modern techniques (like safe languages and modular kernel components) in an accessible way. The novelty lies in how Proto serves as both an educational tool and a research artifact showcasing contemporary OS design choices.

CHERIoT RTOS: An OS for Fine-Grained Memory-Safe Compartments on Low-Cost Embedded Devices. This paper introduces an RTOS for CHERIoT, leveraging CHERI hardware capabilities to enforce memory safety on microcontrollers. The main contribution is a lightweight compartmentalization mechanism that provides fine-grained memory-safe domains on resource-constrained IoT devices ￼. By using CHERI’s hardware-enforced capabilities, the OS isolates components with minimal overhead, dramatically improving security for embedded systems. This demonstrates practical memory-safe OS design on low-cost devices, preventing memory corruption vulnerabilities by hardware-assisted means ￼.

The Design and Implementation of a Virtual Firmware Monitor. The authors propose a novel virtual firmware monitor that runs beneath the OS/hypervisor to supervise device firmware execution. The monitor virtualizes firmware (such as UEFI or device controller code) and intercepts its interactions with hardware. This architecture improves reliability and security by allowing the system to apply updates, enforce policies, or roll back faulty firmware at runtime. The key contribution is demonstrating how firmware – traditionally opaque and unmanaged by the OS – can be brought under hypervisor control, preventing firmware bugs or malware from undermining system stability.

PhoenixOS: Concurrent OS-Level GPU Checkpoint and Restore with Validated Speculation. PhoenixOS enables fast, transparent checkpoint/restore of GPU-accelerated applications at the OS level ￼ ￼. The paper’s novelty is a two-step speculation and validation mechanism: it speculatively tracks GPU memory accesses using launch-time arguments and then efficiently validates them via binary instrumentation ￼. By coordinating GPU kernel state and data transfer, PhoenixOS can take consistent checkpoints of running GPU tasks without long pauses. This allows reliable fault tolerance and migration for GPU computations, demonstrating an OS service for GPU state management that was previously lacking.

How to Copy Memory? Coordinated Asynchronous Copy as a First-Class OS Service. This work introduces Copier, a new OS-level service for high-performance bulk memory copying ￼. Unlike traditional copy mechanisms, Copier coordinates asynchronous memory copy operations between user applications and OS tasks. The key contribution is treating memory copy as a first-class OS citizen: multiple subsystems (user programs, file systems, etc.) can offload large copy jobs to Copier, which schedules and executes them efficiently in the background ￼. This improves throughput by overlapping copy with computation and by optimizing memory bus usage, addressing a fundamental but often ignored OS operation.

FlexGuard: Fast Mutual Exclusion Independent of Subscription. FlexGuard is a synchronization mechanism that dynamically adapts between spin-locking and blocking to achieve high-performance mutual exclusion ￼. Its key novelty is guarded locking: it monitors whether a thread holding a lock is actively running or not, and if not, FlexGuard switches waiting threads from busy-waiting to a blocking state ￼. This independent-of-subscription approach means that lock performance is robust across a variety of thread scheduling scenarios. The result is significantly faster locks under contention, without the need for manual tuning, by combining the low latency of spinlocks with the resource efficiency of mutexes ￼.

Scalable Address Spaces using Concurrent Interval Skiplist. This paper addresses the scalability of virtual address space operations by introducing a concurrent interval skip list data structure for memory management. The novelty is a lock-free skip list that manages address space ranges (intervals) efficiently in parallel ￼ ￼. It replaces traditional VMAs (Virtual Memory Areas) or tree-based structures with a skip list supporting fast concurrent inserts, deletions, and lookups of memory regions. This yields scalable address space operations (e.g., mmap, munmap) on multicore systems, greatly improving performance for multithreaded applications that heavily allocate and free memory ￼.

Mitigating Application Resource Overload with Targeted Task Cancellation. The authors present a runtime mechanism to handle overload in multi-tenant systems by selectively canceling tasks. The system monitors resource saturation (CPU, memory or I/O) and proactively cancels or defers lower-priority tasks to relieve pressure. The key contribution is a targeted cancellation policy guided by online profiling: rather than indiscriminately throttling, it identifies specific tasks causing overload and safely aborts them. This approach improves overall responsiveness and prevents cascading failures under extreme load, demonstrating a practical OS-level strategy for maintaining QoS during overload conditions.

# Storage and File Systems

Aeolia: A Fast and Secure Userspace Interrupt-Based Storage Stack. Aeolia introduces a storage stack that moves I/O processing to userspace and replaces traditional polling with an interrupt-based mechanism ￼. The system allows user-level storage drivers to receive asynchronous notifications (interrupts) for I/O completions, drastically reducing latency. To ensure security, Aeolia sandboxes the user-level storage handlers and uses hardware virtualization extensions to protect kernel data ￼. Its primary contribution is demonstrating that a userspace storage stack can achieve kernel-level performance while providing strong isolation, by carefully handling interrupts and direct device access in a secure manner.

Sleeping with One Eye Open: Fast, Sustainable Storage with Sandman. The Sandman system targets energy efficiency in storage clusters without sacrificing performance. It introduces a new abstraction that allows storage devices (e.g., SSD arrays) to “sleep” (enter low-power modes) when idle while a lightweight monitor thread – the “one eye open” – remains active to quickly handle incoming requests. The novelty is an integrated scheduling and prediction mechanism that preemptively wakes devices based on workload patterns, striking a balance between power savings and response time. Sandman’s experiments show significant energy reduction for storage-intensive applications, demonstrating sustainable storage system design.

Managing Scalable Direct Storage Accesses for GPUs with GoFS. This paper presents GoFS, a GPU-Orchestrated File System that enables GPUs to perform high-throughput direct storage I/O ￼. Unlike conventional systems where the CPU handles file I/O for GPU jobs, GoFS offloads storage management logic to run on the GPU itself ￼. The key contribution is a set of GPU-resident data structures and scheduling techniques that allow many GPU threads to read/write from NVMe storage in parallel without CPU intervention. By doing so, GoFS achieves scalable, low-latency I/O for data-intensive GPU programs, effectively turning the GPU into an active storage client for better end-to-end performance ￼.

cache_ext: Customizing the Page Cache with eBPF. cache_ext introduces an eBPF-based framework to extend and customize the OS page cache behavior ￼. It allows developers to inject small eBPF programs into the kernel’s caching subsystem to implement custom policies (for eviction, prefetching, write-back, etc.) at runtime. The novelty is that these extensions run safely in the kernel via eBPF without modifying kernel code, enabling use-case-specific optimizations. Experiments show that cache_ext can implement policies that improve performance for certain workloads (like database or VM disk access) by tailoring cache management, all while maintaining isolation and stability of the kernel ￼.

Fawkes: Finding Data Durability Bugs in DBMSs via Recovered Data State Verification. Fawkes proposes a technique to uncover durability and recovery bugs in database systems ￼. It works by intentionally crashing a DBMS and then verifying the recovered state against expected invariants ￼. The key idea is to generate operations that stress-test the DBMS recovery logic (e.g., complex transaction sequences, writes, and checkpoints), then use a verification tool to detect any divergence or data loss after crash recovery. Fawkes’s novelty is in automating this crash-and-verify strategy to systematically find corner-case bugs that violate durability guarantees (e.g., missing or corrupted data after a crash) ￼. This helps improve the reliability of database durability mechanisms.

WASIT: Deep and Continuous Differential Testing of WebAssembly System Interface Implementations. WASIT introduces a fuzzing framework to test implementations of the WebAssembly System Interface (WASI) for consistency and security. It generates random but semantically equivalent operations across different WASI runtimes and then compares the outcomes (state, outputs) continuously ￼. By observing divergences in behavior, WASIT can pinpoint bugs or undefined behaviors in WASI implementations. The main contribution is applying differential testing in a deep, ongoing fashion to the WebAssembly OS interface, uncovering issues in how various runtimes handle system calls. This work improves the robustness of WebAssembly as a safe, portable computing platform by hardening its system interface.

# Memory and Disaggregated Systems

Oasis: Pooling PCIe Devices Over CXL to Boost Utilization. Oasis provides a mechanism to dynamically pool and share PCIe devices (like accelerators or SSDs) across multiple hosts via the CXL interconnect ￼. It contributes a control plane and data path for remapping PCIe traffic over CXL, effectively allowing one machine to use devices physically attached to another ￼. The key novelty is handling the challenges of routing device memory and interrupts across host boundaries with low overhead. Oasis’s pooled CXL approach significantly improves utilization by letting underused devices be lent to other machines on demand, demonstrating a practical step toward composable disaggregated hardware in data centers ￼.

Spirit: Fair Allocation of Interdependent Resources in Remote Memory Systems. Spirit addresses the problem of allocating resources in memory disaggregation setups, where applications consume both local and remote memory (and associated network and CPU resources). It introduces a fair scheduling algorithm that accounts for the interdependence of resources – for example, an application using remote memory also uses network bandwidth and remote CPU on the memory server. Spirit’s novelty is a fairness model that weights these coupled resources to avoid starving any component ￼ ￼. The main contribution is ensuring predictable performance in remote memory systems by preventing any one app from monopolizing the combined memory–network–compute pipeline, thus maintaining fairness and efficiency.

Scalable Far Memory: Balancing Faults and Evictions. This work tackles the reliability-performance tradeoff in far-memory systems (where “far” memory on remote servers extends local memory). It introduces a system that jointly manages fault tolerance and cache eviction for far memory. The novelty is a policy that tolerates a certain rate of faults (remote node failures) by replicating or encoding data across memory servers, while also optimizing eviction of rarely used data to secondary storage. By treating faults and evictions in a unified cost model, the system can “balance” them ￼ – minimizing data loss from server failures without over-burdening the network or local memory with excessive replication. This approach yields far-memory caches that are both highly durable and high-performance under large-scale conditions.

Demeter: A Scalable and Elastic Tiered Memory Solution for Virtualized Cloud via Guest Delegation. Demeter presents a tiered-memory management framework in cloud VMs that delegates some control to the guest OS ￼. It coordinates the hypervisor and guest to manage different memory tiers (e.g., DRAM, NVM, remote memory) in an elastic manner. The key idea is guest delegation: the guest OS is made aware of memory tiers and participates in decisions (such as which pages to migrate or evict) rather than the hypervisor alone. This collaboration allows more efficient use of fast and slow memory tiers at scale. Demeter’s experiments show improved performance and elasticity (on-demand memory growth/shrink) by reducing virtualization overhead – the guest’s insight into workload needs leads to smarter memory tier management ￼.

Orthrus: Efficient and Timely Detection of Silent User Data Corruption in the Cloud with Resource-Adaptive Computation Validation. Orthrus introduces a runtime system to detect silent data corruption (SDC) in cloud computations ￼. SDCs (e.g., due to memory errors or CPU bugs) do not cause crashes but silently produce incorrect results. Orthrus’s novelty is a resource-adaptive validation mechanism: it runs redundant computations or checksums in parallel with normal execution, adjusting the level of checking based on available resources. If the system is lightly loaded, Orthrus performs deeper validation; under heavy load, it scales back to avoid overhead. This adaptive approach catches SDCs with minimal performance impact, providing timely detection of otherwise undetectable data corruptions in user applications and thereby improving cloud reliability.

CortenMM: Efficient Memory Management with Strong Correctness Guarantees. CortenMM is a memory management system that aims to be both high-performance and formally correct ￼. It introduces a new allocator algorithm that is optimized for multithreaded scalability while providing provable safety properties (like no use-after-free or double-free). The key contribution is combining low-level efficiency tricks (such as slab allocation and bulk freeing) with machine-checked proofs of correctness. In essence, CortenMM offers the speed of state-of-the-art memory allocators and memory management (MM) schemes, but with the added assurance that its operations satisfy strict correctness invariants under concurrency. This gives strong reliability guarantees without sacrificing performance ￼.

# Cloud Computing and Resource Management

COpter: Efficient Large-Scale Resource-Allocation via Continual Optimization. COpter reconceptualizes cloud resource allocation as a never-ending optimization problem rather than periodic static assignments ￼. It treats resource allocation (for CPU, memory, etc.) as a continuously adjusting linear programming (LP) that evolves with workload changes. The novel idea is continual optimization: instead of solving from scratch for each scheduling interval, COpter updates the previous solution incrementally as demands shift ￼. This approach yields near-optimal allocations much faster than recomputation. The paper’s contribution is demonstrating that a sequence of interconnected optimization problems can be efficiently solved in an online manner, significantly improving allocation agility and utilization in large clusters ￼.

Moirai: Optimizing Placement of Data and Compute in Hybrid Clouds. Moirai tackles the challenge of partitioning workloads between on-premise data centers and the public cloud ￼. It introduces a scheduler that jointly considers data location and compute availability to decide where to run each component of a job (e.g., keep data-heavy tasks on-premises near storage, send bursty compute-heavy tasks to cloud). The novelty lies in an optimization model that balances cost, latency, and data transfer: Moirai can, for example, decide to move data to cloud if network costs are low, or keep compute local to avoid large data egress fees ￼. By dynamically adapting placement as conditions change, Moirai improves performance and cost-efficiency for hybrid cloud deployments, automating a traditionally manual decision process.

Quilt: Resource-Aware Merging of Serverless Workflows. Quilt improves the efficiency of serverless computing workflows by automatically merging functions across the workflow DAG into consolidated units ￼. The key observation is that serverless applications often consist of many small functions that incur high invocation and communication overheads. Quilt’s compiler analyzes a workflow and merges compatible functions (even if written in different languages) into a single process, eliminating intermediate calls ￼. This resource-aware merging respects memory and CPU constraints to not over-pack a process. The result is significantly reduced cold starts, network transfers, and invocation latencies for multi-function workflows, achieved without developer effort by this automated merge strategy ￼.

Dandelion: Unlocking True Elasticity for the Cloud-Native Era. Dandelion proposes an elastic cloud platform with a declarative programming model that rethinks how cloud applications are built and scaled ￼. Instead of traditional VM/container interfaces (POSIX sockets, etc.), Dandelion has applications describe their needs as DAGs of compute and communication functions, using higher-level service interfaces like HTTP-based storage or database calls ￼. The platform can then transparently scale, migrate, and transform these DAG components across the cloud. The main contribution is replacing low-level network interfaces with intent-based declarations, allowing the runtime to provide “true elasticity” – seamlessly scaling resources up/down and optimizing placement without violating application correctness ￼. This empowers cloud-native apps to achieve both high performance and minimal resource usage by leveraging the platform’s global optimizations.

Loom: Efficient Capture and Querying of High-Frequency Telemetry. Loom is a telemetry system designed for cloud infrastructure that must collect and analyze metrics at extremely high frequencies (e.g., per-millisecond events) ￼. Its novelty is a combined data pipeline that interweaves capture and querying: as it gathers telemetry data, it partially aggregates and indexes it on the fly, enabling near-real-time queries. Loom introduces new data structures for efficient time-series compression and retrieval under heavy write loads. The key contribution is achieving scalable ingestion (so the “loom” can weave vast streams of metrics) while still allowing timely queries on recent data. This helps operators detect anomalies or trends almost immediately, even at massive scale.

Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks. Tiga addresses the long latencies of distributed transactions across distant data centers by leveraging synchronized clocks ￼. It introduces a protocol that uses tightly synchronized physical clocks (e.g., via modern GPS or atomic clocks) to commit transactions more efficiently across sites. The core idea is to assign globally consistent timestamps without extensive cross-site communication, thus reducing coordination overhead. By using clock synchronization, Tiga can safely order transactions and detect conflicts across geographically distributed nodes with minimal waiting. The result is a significant speedup for geo-distributed databases, as transactions commit in one round-trip instead of multiple, demonstrating the power of clock synchronization for distributed consistency ￼.

Pesto: Cooking up High Performance BFT Queries. Pesto is a system for executing queries in a Byzantine Fault Tolerant (BFT) database or service with high throughput ￼. It introduces a set of optimizations (“ingredients”) that drastically reduce the overhead typically associated with BFT consensus for each query. This includes batching queries, speculative execution, and lightweight verification mechanisms that cut down cryptographic costs. The key contribution is showing that BFT systems, often considered too slow for high-performance use, can indeed support demanding workloads by carefully redesigning the query processing pipeline ￼. Pesto achieves much lower latency and higher throughput for read/write operations under Byzantine assumptions, making robust distributed systems more practical.

# Security and Reliability

ORQ: Complex Analytics on Private Data with Strong Security Guarantees. ORQ is a system enabling rich data analytics on sensitive data while preserving privacy and security ￼. It provides an analytical engine that operates on encrypted or access-restricted data without exposing raw values. The novelty is a combination of cryptographic techniques (like secure multi-party computation or homomorphic encryption) with query optimization to handle complex queries (ORQ stands for “Analytics on pRivate Data with strong Quality”) efficiently. The result is that organizations can run sophisticated analytics (e.g., SQL queries, machine learning) on joint datasets from multiple parties or on encrypted databases, getting accurate results with provable security that the raw private data remains undisclosed. ORQ demonstrates that strong security need not come at the cost of analytic expressiveness ￼.

eBPF Misbehavior Detection: Fuzzing with a Specification-Based Oracle. This paper presents a new fuzz-testing framework to find bugs and misbehavior in eBPF programs and implementations ￼. It uses a specification-based oracle: the authors formalize the expected behavior of eBPF instructions and helpers, then fuzz both the eBPF verifier and runtime with random programs to see if the outcomes deviate from the spec ￼. The key innovation is combining fuzzing with a rigorous semantic oracle, which can catch subtle verifier acceptance bugs or runtime faults where eBPF programs do something insecure (like escaping sandbox). This approach uncovered multiple discrepancies and vulnerabilities in eBPF across different kernel versions ￼, helping to harden this critical in-kernel virtualization technology.

Prove It to the Kernel: Precise Extension Analysis via Proof-Guided Abstraction Refinement. This work introduces a framework to formally verify kernel extensions (such as loadable modules or eBPF programs) with a high degree of automation ￼. The key idea is proof-guided abstraction refinement: it attempts to prove safety properties of an extension, and if the proof fails, it automatically refines the abstract model of the kernel until the proof succeeds ￼. By iteratively tightening the model only where needed, it achieves precise analysis without excessive manual effort. The main contribution is ensuring that extensions “prove” to the kernel they are safe – e.g., no illegal pointer dereferences or privilege escalations – before loading, thereby greatly increasing OS reliability ￼. This approach was evaluated on real kernel extensions, catching bugs and proving correctness for those that passed.

Ghost in the Android Shell: Pragmatic Test-Oracle Specification of a Production Hypervisor. This paper focuses on systematically testing a production mobile hypervisor (the “Android shell”) by developing pragmatic test oracles ￼. The authors derive specifications (expected behaviors) for the hypervisor’s key functions and use those as oracles to check test outcomes. The novelty is in balancing formality and practicality: the specs are detailed enough to catch subtle bugs, but not so formal that they can’t be applied to an existing commercial hypervisor. Using these oracles, the team identifies mismatches between the hypervisor implementation and the specified behavior (the “ghosts” in the shell). The contributions are twofold: a methodology for spec-driven testing of low-level system software, and the bugs uncovered in a widely deployed hypervisor, demonstrating the approach’s effectiveness ￼.

Atmosphere: Practical Verified Kernels with Rust and Verus. Atmosphere describes the design of an OS kernel that is formally verified for safety properties yet written in a practical systems language (Rust) ￼. Using Verus (a verification tool for Rust), the authors verify crucial kernel components like thread scheduling, memory isolation, and IPC, proving the absence of certain classes of bugs. The novelty is achieving verification on a realistic kernel without sacrificing performance or needing a bespoke language – instead, they leverage Rust’s safety and translate proofs through Verus for the tricky parts (like unsafe code). Atmosphere demonstrates that modern verification can integrate with systems programming, yielding a kernel that is both efficient and accompanied by machine-checked proofs of its key correctness and security properties ￼.

TickTock: Verified Isolation in a Production Embedded OS. TickTock is an embedded operating system that has been formally verified to provide strong isolation between components ￼. It runs on real hardware and is used in production (e.g., IoT devices). The paper details how the team applied formal methods to verify the scheduler and memory subsystem of an existing OS (rather than building a new verified OS from scratch). The main contribution is the technique of post-facto verification – proving properties about an existing codebase, aided by some retrofitting of invariants – to show that even if multiple tasks run and devices interrupt, isolation holds (no data leaks or corruption across tasks). TickTock proves that rigorous verification can be achieved in a production setting, increasing trust in deployed embedded systems ￼.

AutoMan: Facilitating Verified Distributed Systems Development Through Automatic Code Generation and Manual Optimizations. AutoMan is a framework that aids developers in building formally verified distributed systems by automatically generating large portions of the system code from high-level specifications ￼. It produces a correct-by-construction baseline which developers can then manually optimize where needed. The critical novelty is that AutoMan ensures any manual optimizations do not violate correctness: it provides tooling to verify that the optimized code refines the original spec. The paper demonstrates this with a case study where a verified baseline is generated and then hand-tuned for performance. The end result is a distributed system that is both efficient and has end-to-end proofs of its key properties, achieved with significantly less verification effort than doing everything manually ￼.
